29 | 微服务如何实现DevOps？

1）什么是DevOps？

CI（Continuous Integration），持续集成。开发完成代码开发后，能自动地进行代码检查、单元测试、打包部署到测试环境，进行集成测试，跑自动化测试用例。
CD（Continuous Deploy），持续部署。代码测试通过后，能自动部署到类生产环境中进行集成测试，测试通过后再进行小流量的灰度验证，验证通过后代码就达到线上发布的要求了，就可以把代码自动部署到线上。


2）一个服务的发布流程主要包含了三个步骤
1.持续集成，这个步骤的主要作用是确保每一次代码的Merge Request都测试通过，可随时合并到代码的Develop分支，主要包括四个阶段：build阶段（开发分支代码的编译与单元测试）、package阶段（开发分支代码打包成Docker镜像）、deploy阶段（开发分支代码部署到测试环境）、test阶段（开发分支代码集成测试）。
2.持续交付，这个步骤的主要作用是确保所有代码合并Merge Request到Develop分支后，Develop分支的代码能够在生产环境中测试通过，并进行小流量灰度验证，可随时交付到线上。主要包括五个阶段：build阶段（Develop分支的代码编译与单元测试）、package阶段（Develop分支的代码打包成Docker镜像）、deploy阶段（Develop分支的代码部署到测试环境）、test阶段（Develop分支的代码集成测试）、canary阶段（Develop分支的代码的小流量灰度验证）。
3.持续部署，这个步骤的主要作用是合并Develop分支到Master主干，并打包成Docker镜像，可随时发布到线上。主要包括四个阶段：build阶段（Master主干的代码编译与单元测试）、package阶段（Master主干的代码打包成Docker镜像）、clear阶段（Master主干的代码Merge回Develop分支）、production阶段（Master主干的代码发布到线上）。

3）实现DevOps的关键点
    持续集成阶段
        代码检查。通过代码检查可以发现代码潜在的一些bug，比如Java对象有可能是null空指针等，实际执行时可以在持续集成阶段集成类似[Sonarqube](https://www.sonarqube.org/)之类的工具来实现代码检查。
        单元测试。单元测试是保证代码运行质量的第二个关卡。单元测试是针对每个具体代码模块的，单元测试的覆盖度越高，各个代码模块出错的概率就越小。不过实际业务开发过程中，为了追求开发速度，许多开发者并不在意单元测试的覆盖度，而是把大部分测试工作都留在了集成测试阶段，这样可能会造成集成测试阶段返工的次数太多，需要多次修复bug才能通过集成测试。尤其对于业务复杂度比较高的服务来说，在单元测试阶段多花费一些功夫，其实从整个代码开发周期角度来看，收益还是要远大于付出的。
        集成测试。集成测试就是将各个代码的修改集成到一起，统一部署在测试环境中进行测试。为了实现整个流程的自动化，集成自测阶段主要的任务就是跑每个服务的自动化测试用例，所以自动化测试用例覆盖的越全，集成测试的可靠性就越高。这里就要求开发和测试能及时沟通，在新的业务需求确定时，就开始编写测试用例，这样在跑自动化测试用例时，就不需要测试的介入了，省去了沟通成本。当然，业务开发人员也可以自己编写测试用例，这样的话就不需要专职的业务测试人员了。
    持续交付阶段
        持续交付阶段的主要目的是保证最新的业务代码，能够在类生产环境中可能够正常运行，一般做法都是从线上生成环境中摘掉两个节点，然后在这两个节点上部署最新的业务代码，再进行集成测试，集成测试通过后再引入线上流量，来观察服务是否正常。
    持续部署阶段
        持续部署阶段的主要目的把在类生产环境下运行通过的代码自动的发布到线上所有节点中去，这里的关键点就在于实际的线上发布阶段并不是想象中的那么直接


30 | 如何做好微服务容量规划？
1）拆分微服务的复杂度主要来自下面几个方面
    服务数量众多，纯靠人肉运维难以管理，比如微博Feed业务仅仅RPC服务就有将近40个。
    服务的接口表现差异巨大，有的接口属于访问量比较大，但接口响应时间比较短的轻接口；有的接口属于访问量比较小，但接口响应时间比较长的重接口。比如微博Feed业务中计数接口的平均耗时只有2～3ms，而微博Feed业务中Feed接口的平均耗时要超过200ms。
    服务部署的集群规模大小不同，需要扩容的机器数量差异很大。比如微博的AB测试服务集群只有大约20台机器，扩容只需要几台机器就满足了；而Feed服务则有上千台机器，往往扩容需要上百台机器。
    服务之间还存在依赖关系，在服务扩容的时候，还需要考虑依赖服务的容量是否足够。比如微博Feed业务扩容还依赖用户关系服务和Card服务，扩容时还需要考虑依赖的用户关系服务和Card服务容量是否有问题。

2）容量规划系统的作用
    根据各个微服务部署集群的最大容量和线上实际运行的负荷，来决定各个微服务是否需要弹性扩缩容，以及需要扩缩容多少台机器。

3）容量评估
    选择合适的压测指标：
        系统类指标：机器的CPU使用率、内存占用量、磁盘I/O使用率以及网卡带宽
        服务类指标：接口响应的平均耗时、P999耗时、错误率
        接口的慢速比：也就是接口响应时间高于某个阈值的比例。
    压测获取单机的最大容量：
        单机压测一般有两种方式，一种是通过日志回放等手段，模拟线上流量来对单机进行压测；一种是通过TCP-Copy的方式，把线上机器的流量拷贝过来对单机进行压测。
        集群压测是对整个集群进行压测，以获取单机的最大容量。一般做法是通过不断把线上集群的节点摘除，以减少机器数的方式，来增加线上节点单机的流量，从而达到压测的目的。
    实时获取集群的运行负荷：
        通过压测能够获取到单机的最大容量，再乘以集群内的机器数量就是集群的最大容量了，下一步获取集群实际运行的负荷，就可以判断集群是否需要扩容了。
        跟刚才计算单机容量的方式类似，集群的运行负荷也需要通过采用区间加权的方式来计算，但是因为集群的规模可能很大，超过上千台机器，
        显然通过计算每台单机运行的负荷再加在一起的方式效率不高。我在线上实际使用的方法是统计每台单机在不同耗时区间内的请求数，推送到集中处理的地方进行聚合，
        将同一个集群内的单机位于不同耗时区间内的请求进行汇总，就得到整个集群的请求在不同耗时区间内的分布了，再利用区间加权的方式就可以计算整个集群的运行负荷。

4）调度决策
    水位线：我在实际线上业务使用的是水位线来进行调度决策。

5）执行扩缩容时，机器数量该如何决定
    扩容：在决定扩多少机器时，一般有两种方式，一种是按数量，一种是按比例
    缩容：然后按照10%、30%、50%、100%的比例进行缩容

31 | 微服务多机房部署实践

1）多机房部署问题
    一切正常时用户请求该访问哪个机房？
    多个机房之间的数据如何同步？
    多个机房之间的数据如何确保持一致性？

2）多机房负载均衡
    就近原则、需要调配流量，达到各个机房流量均衡的目的

3）多机房数据同步
    主从机房架构：主从机房架构是以一个机房为主机房，所有的写请求都只发给主机房的处理机
    独立机房架构：（图31-01）
        WMB消息同步组件的功能就是把一个机房的写请求发给另外一个机房，它的实现原理可以用下面这张图来描述，分为两个部分：
        reship，负责把本机房的写请求分发一份给别的机房。
        collector，负责从别的机房读取写请求，然后再把请求转发给本机房的处理机。
    MCQ消息队列实现：
        下面这张图是采用MCQ消息队列的实现方案，从图中你可以看到联通机房的写请求写入到联通机房的MCQ里，然后联通机房的reship就从联通机房的MCQ里读取，再写入到电信机房的MCQ里
    RPC调用实现
        联通机房的写请求会调用联通机房的reship RPC，然后联通机房的reship RPC就会调用电信机房的collector RPC
4）多机房数据一致性
    强一致性、最终一致性

32 | 微服务混合云部署实践
1）跨云服务的负载均衡
    把用户的访问按照DNS解析到不同的机房，私有云机房部署了VIP和Nginx分别用作四层和七层的负载均衡，阿里云机房部署了SLB和Nginx分别用作四层和七层的负载均衡。
2）跨云服务的数据同步
    私有云与公有云之间的网络隔离:需要架设专门的VPN网络或者专线
    数据库能否上云
3）跨云服务的容器运维
    跨云的主机管理
    跨云服务发现
    跨云弹性扩容
    跨云服务编排


